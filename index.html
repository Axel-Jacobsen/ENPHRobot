<!DOCTYPE html>
<html>

<head>
    <link rel="stylesheet" href="styles/css/main.css">
    <title>Vanessa</Title>
    <style>
        @import url('https://fonts.googleapis.com/css?family=Lato');
    </style>
</head>

<body>
    <div class="pageDiv introDiv">
        <div class="shade positionTopLeft">
            <h2 class="titleCard">Vanessa</h2>
            <p class="mainLinks">
                <a href="#theproblem">The problem</a>
                <a href="#strategy">Strategy</a>
                <i>Major Components</i>
                <a href="#mechanical">Mechanics</a>
                <a href="#electrical">Electronics</a>
                <a href="#software">Software</a>
                <a href="#projectmgmt">Project Management</a>
                <i>Final Notes</i>
                <a href="#comp">Performance and Takeaways</a>
                <a href="#whoami">Meet the team</a>
            </p>
        </div>
    </div>

    <div class="pageDiv problemDiv" id="theproblem">
        <div class="vFlexOnWidth">
            <div class="halfWidth">
                <h2 class="titleCard">The Problem</h2>
                <p>
                    Four Ewoks and Chewbacca are trapped in the Empire Stronghold! Our task was to create an autonomous robot to navigate the
                    stronghold and return the prisoners to the start of the course.
                </p>
                <p>
                    To do this, we had to navigate over gaps, identify and react to alternating IR frequencies, climb a ramp, and deploy a basket
                    to return our captured critters.
                </p>
                <p>
                    This is a diagram of the competition surface. It shows the positions of the Ewoks and Chewbacca, the gaps that we had to
                    cross, and the position and direction of the IR sensors.
                </p>
            </div>

            <!-- We should change this image if we can, leaving it now to finish other stuff-->
            <img class="baseImg compImg" src="images/competition.png">
        </div>
    </div>

    <div class="pageDiv strategyDiv" id="strategy">
        <div class="vFlexOnWidth vReverseColumn">
            <img class="baseImg robotImg" src="images/realbot.JPG">
            <div class="halfWidth rightSideText">
                <h2 class="titleCard">Strategy</h2>
                <p>
                    Our team took an unique approach to this challenge.
                </p>
                <p>
                    To navigate to Ewoks, a RaspberryPi was running a machine learning algorithm called
                    <a href="https://pjreddie.com/darknet/yolo/">YOLO</a> to find their locations.
                </p>
                <p>
                    This RaspberryPi would send signals to a microcontroller (STM32F4) that was processing signals and making decisions. The
                    microcontroller would request directions to an Ewok when it was ready, allowing us to navigate purely
                    by finding Ewoks, and not by following tape.
                </p>
                <p>
                    We chose a treaded robot in order to traverse the gaps. This let us roll over the gaps instead of taking time to deploy a
                    bridge that may be misplaced. Treads gave our robot reliability, as well as allowing us to focus on other
                    aspects of the competition.
                </p>
            </div>
        </div>
    </div>


    <div class="pageDiv mechDiv" id="mechanical">
        <div class="vFlexOnWidth">

        </div>
    </div>

    <div class="pageDiv elecDiv" id="electrical">
        <div class="vFlexOnWidth vReverseColumn">
            <img class="baseImg pcbImg" src="images/board.png">
            <div class="halfWidth rightSideText">
                <h2 class="titleCard">Electronics</h2>
                <p>
                    The electrical circuitry of the robot was all integrated into a single printed circuitboard, which we designed ourselves
                    using Autodesk Eagle. Printing the circuit board allowed us to bundle up the "brains" of our robot in
                    a small volume able to fit between the two tracks, making our design compact, yet still robust. We blew
                    some traces powering our motors a few times. We also had to make the circuits we designed somewhat adaptable
                    on the circuit board, in case a design changed during the development of our robot. To do this, we used
                    common IC packages as much as possible, and kept them all at the same power and ground rails. Our single
                    circuit board was comprised of two h-bridge motor drivers; a switching voltage regulator and a couple
                    of resistors which were used to control all the servos in the robot (the basket and claw mechanisms);
                    another switching voltage regulator to power the STM32 F4 MCU and Raspberry Pi (which fitted into the
                    board as if it were a giant shield); a linear voltage regulator to provide a separate clean power supply
                    for the ICs; two rail-to-rail amplifiers which were used to amplify infrared signals for the MCU to process
                    digitally; and four opto-sensor comparator circuits which use infrared transmission and reflection to
                    detect edges and nearby obstacles; and another infrared beam which tripped when an ewok entered the claw
                    and alerted us to capture it. We were able to placate any potential noise issues by using a common ground
                    plane throughout the circuit board, placing a pair of decoupling capacitors at every IC, and by powering
                    the three main parts of the circuit board separately. The motor drivers and MCUs were powered by the
                    main battery. The servos were powered by another battery, and the ICs were powered by a separate battery.
                    Issues: some of the trace widths were too small, and blew when high current past through them (i.e. motors).
                    We also found that under harsh conditions (lifting the robot so there is no load on the motors when driven
                    fully) created ground transience which travelled back into the STM and fried it. To fix this issue, we
                    added a zener diode and capacitor in parallel on the input supply to prevent power spikes from frying
                    it. We also added parallel shocky diodes on the hbridge signals try isolate them. (edited) we can also
                    get diagrams of the circuits from eagle if we want to describe how each of them work individually
                </p>
            </div>
        </div>
    </div>

    <div class="pageDiv codeDiv" id="software">
        <div class="vFlexOnWidth">
            <div class="vFlexOnWidth column verticalImageAndCaptions">
                <h4>Onboard images from competition</h4>
                
                <img class="baseImg compImg" src="images/comp1.jpg">
                <i>First image near the end of the ramp.</i>

                <img class="baseImg compImg" src="images/comp2.jpg">
                <i>Second image, just before the first Ewok. The horizontal lines are due to damage to the ribbon cable from the camera to the Rasberry Pi.</i>

                <img class="baseImg compImg" src="images/comp3.jpg">
                <i>Third image, right after the first gap. The miscoloration is due again to damage to the damage to the camera's ribbon cable. Notice, even with the color distoriton, the network still found the Ewok.</i>
            </div>
            <div class="halfWidth rightSideText">
                <h2 class="titleCard">Software</h2>
                <p>
                    In order to navigate towards the Ewoks, we used computer vision. A Raspberry Pi on the robot continuously captured images
                    of the course ahead using a fisheye camera, and these photos were processed using a Python implementation
                    of the extremely fast YOLO network (paper | website) trained on over 2500 photos of the competition surface.
                    During a run, the STM microcontroller would periodically request images from the Raspberry Pi. When it
                    did, the network would find the coordinates of the bounding boxes around each Ewok in the frame, and
                    we would calculate our heading error by finding the distance between the center of the frame and the
                    center of the bounding box. This error was then written to an 8-bit digital to analog converter (a resistor
                    ladder on our PCB) and the analog voltage was read by the STM. At the same time, an interrupt was triggered
                    on the STM, telling it to stop the normal navigation loop, and turn to the heading indicated by the Raspberry
                    Pi. This would turn the robot to face the next Ewok, and then the robot would continue driving towards
                    it. By using computer vision, we were able to see the Ewoks much farther ahead than many of the other
                    teams who were using pressure sensors or infrared light sensors. Because of this, we had to hard-code
                    very few sections of the course, and could navigate directly from Ewok to Ewok. Given more time, a universal
                    procedure with no hard-coded maneuvers could have been used to navigate any course with similar objectives,
                    without needing prior knowledge of the layout. Issues: The camera still occasionally missed Ewoks in
                    the frame, and the low framerate (~2.5s per frame) required the robot to take things very slowly during
                    the run. Even more training photos could have helped to decrease the number of missed Ewoks, but for
                    the network we were using, running on a Raspberry Pi we likely couldnâ€™t improve on the framerate. A similar
                    approach using a simpler network could have given more speed with the tradeoff of accuracy, which may
                    have been a slight improvement on course.
                </p>
            </div>
        </div>
    </div>

    <div class="pageDiv compDiv" id="comp">
        <div class="vFlexOnWidth">
    
        </div>
    </div>

    <div class="pageDiv whoAreWeDiv" id="whoami">
        <div class="vFlexOnWidth column">
            <h2 class="titleCard">Meet Team 13</h2>
            <div class="vFlexOnWidth autoHeight makeMeThicc">
                <div class="human">
                    <i>Ben Holmes</i>
                    <img class="baseImg humanImg" src="images/ben.jpeg">
                    <p class="humanText">
                        - Machine Learning
                        <br> - Mechanics
                    </p>
                </div>
                <div class="human">
                    <i>Ro-ee Tal</i>
                    <img class="baseImg humanImg" src="images/roee.jpeg">
                    <p class="humanText">
                        - PCB / Electronics
                        <br> - Software
                    </p>
                </div>
                <div class="human">
                    <i>Sean Cameron</i>
                    <img class="baseImg humanImg" src="images/sean.jpeg">
                    <p class="humanText">
                        - CAD / Prototyping
                        <br> - Mechanics
                    </p>
                </div>
                <div class="human">
                    <i>Axel Jacobsen</i>
                    <img class="baseImg humanImg" src="images/axel.jpeg">
                    <p class="humanText">
                        - IR / Electronics
                        <br> - Software
                    </p>
                </div>
            </div>
        </div>
    </div>

    <!--
        <video width="!!!!" height="!!!!" autoplay>
            <source src="IMG_2018.TRIM.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    -->
</body>

</html>